{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b67a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import polars as pl\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import operator\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cee566",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [r'C:\\Users\\Joshh\\Projects\\Stocks\\Data\\Webscraping\\News Scaper\\ICInsider', r'C:\\Users\\Joshh\\Projects\\Stocks\\Data\\Webscraping\\News Scaper\\JamaicaGleaner'\n",
    "           r'C:\\Users\\Joshh\\Projects\\Stocks\\Data\\Webscraping\\News Scaper\\JIS', r'C:\\Users\\Joshh\\Projects\\Stocks\\Data\\Webscraping\\News Scaper\\JamaicaObserver\\business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81142015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining longer lists for a meaningful performance comparison\n",
    "a = list(range(1, 100001))\n",
    "b = list(range(1, 100001))\n",
    "\n",
    "# Perform multiple measurements for accuracy\n",
    "num_iterations = 50\n",
    "mapt = []\n",
    "loopt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c4636c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the performance of the map function\n",
    "for _ in range(num_iterations):\n",
    "    t1 = time.time()\n",
    "    result = list(map(operator.mul, a, b))\n",
    "    t2 = time.time()\n",
    "    mapt.append(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59922dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the performance of the for loop\n",
    "for _ in range(num_iterations):\n",
    "    t1 = time.time()\n",
    "    result = [a[i] * b[i] for i in range(len(a))]\n",
    "    t2 = time.time()\n",
    "    loopt.append(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "009f384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Function Implementation\n",
    "def process_file_with_article(file, shared_article):\n",
    "    \"\"\"Process a single file using a shared article object\"\"\"\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Set the HTML content directly\n",
    "    shared_article.html = html_content\n",
    "    shared_article.download_state = 2\n",
    "    shared_article.parse()\n",
    "    shared_article.nlp()\n",
    "    \n",
    "    return shared_article.text\n",
    "\n",
    "def get_sentiment_map(files):\n",
    "    # Initialize newspaper\n",
    "    testUrl = 'https://www.jamaicaobserver.com/2025/04/27/fa-cup-glory-wont-salvage-man-citys-troubled-season-guardiola'\n",
    "    article = newspaper.Article(testUrl)\n",
    "    \n",
    "    # Process files with map\n",
    "    # Create a partial function with the shared article object\n",
    "    process_with_shared_article = partial(process_file_with_article, shared_article=article)\n",
    "    \n",
    "    # Use map to process files\n",
    "    articles = list(map(process_with_shared_article, files[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6e3522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs4 = get_sentiment_map(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eabcf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time of cycle: 0.005043 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from itertools import cycle\n",
    "\n",
    "# Assuming you have already defined a, b, and num_iterations\n",
    "iter_times = []\n",
    "\n",
    "# Create a cycling iterator for range(num_iterations)\n",
    "cycle_iter = cycle(range(num_iterations))\n",
    "\n",
    "# Take only the required number of iterations\n",
    "for _ in range(num_iterations):\n",
    "    i = next(cycle_iter)  # Get the next value from the cycle\n",
    "    \n",
    "    t1 = time.time()\n",
    "    result = [a[i] * b[i] for i in range(len(a))]\n",
    "    t2 = time.time()\n",
    "    iter_times.append(t2 - t1)\n",
    "\n",
    "avg_iter = sum(iter_times) / num_iterations\n",
    "\n",
    "print(f\"Average time of cycle: {avg_iter:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b7ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time of map: 0.002621 seconds\n",
      "Average time of for loop: 0.004583 seconds\n"
     ]
    }
   ],
   "source": [
    "# Calculate average times\n",
    "avg_mapt = sum(mapt) / num_iterations\n",
    "avg_loopt = sum(loopt) / num_iterations\n",
    "\n",
    "print(f\"Average time of map: {avg_mapt:.6f} seconds\")\n",
    "print(f\"Average time of for loop: {avg_loopt:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b331e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(folderLoc):\n",
    "    ''''This function accepts a list of folders and returns a list of html file paths'''\n",
    "\n",
    "    allFiles = []\n",
    "\n",
    "    folders = folderLoc\n",
    "\n",
    "    for folder in folders:\n",
    "        folder = Path(folder)\n",
    "        folderFiles = list(folder.glob('**/*.html'))\n",
    "\n",
    "        allFiles.append(folderFiles)\n",
    "\n",
    "    # Filter out folders\n",
    "    filesOnly = [f for f in folderFiles if f.is_file()]\n",
    "\n",
    "    return filesOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c5e3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get articles and perform sentiment analysis\n",
    "def get_sentiment(files: list) -> list:\n",
    "    # Initialize newspaper\n",
    "    testUrl = 'https://www.jamaicaobserver.com/2025/04/27/fa-cup-glory-wont-salvage-man-citys-troubled-season-guardiola'\n",
    "    article = newspaper.Article(testUrl)\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for file in files[:100]:\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            html_content = f.read()\n",
    "\n",
    "        # Set the HTML content directly\n",
    "        article.html = html_content\n",
    "        article.download_state = 2\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "\n",
    "        articles.append(article.text)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9237354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38434963",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_sentiment(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b6756bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_sentiment2(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "810efc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0be3ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"Process a single file and return the article text\"\"\"\n",
    "    # Create a new Article object for each thread to avoid conflicts\n",
    "    article = newspaper.Article('')\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            html_content = f.read()\n",
    "        \n",
    "        # Set the HTML content directly\n",
    "        article.html = html_content\n",
    "        article.download_state = 2\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {file_path}: {str(e)}\"\n",
    "\n",
    "def get_sentiment2(files: list) -> list:\n",
    "    \"\"\"Get articles and perform sentiment analysis using parallel processing\"\"\"\n",
    "    # Determine optimal number of workers based on CPU cores\n",
    "    # Use max(4, CPU_COUNT) to ensure at least 4 threads for I/O-bound tasks\n",
    "    num_workers = max(4, multiprocessing.cpu_count())\n",
    "    \n",
    "    # Only process first 10 files as in the original function\n",
    "    files_to_process = files[:100]\n",
    "    results = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O-bound operations\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Submit all file processing tasks\n",
    "        future_to_file = {executor.submit(process_single_file, file): file for file in files_to_process}\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in future_to_file:\n",
    "            try:\n",
    "                text = future.result()\n",
    "                results.append(text)\n",
    "            except Exception as exc:\n",
    "                print(f'File processing generated an exception: {exc}')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14c4939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_file_batch(file_batch):\n",
    "    \"\"\"Process a batch of files with a single Article object\"\"\"\n",
    "    article = newspaper.Article('')\n",
    "    results = []\n",
    "    \n",
    "    for file in file_batch:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            # Reset article state\n",
    "            article.html = html_content\n",
    "            article.download_state = 2\n",
    "            article.is_parsed = False\n",
    "            article.is_downloaded = True\n",
    "            \n",
    "            # Process\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            \n",
    "            results.append(article.text)\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error processing {file}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_sentiment3(files: list) -> list:\n",
    "    \"\"\"Get articles and perform sentiment analysis with optimized threading\"\"\"\n",
    "    # Limit to 10 files as in the original\n",
    "    files_to_process = files[:100]\n",
    "    \n",
    "    # For small numbers of files, threading might not be beneficial\n",
    "    if len(files_to_process) <= 3:\n",
    "        # Use the original approach for small batches\n",
    "        article = newspaper.Article('')\n",
    "        results = []\n",
    "        \n",
    "        for file in files_to_process:\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            article.html = html_content\n",
    "            article.download_state = 2\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            \n",
    "            results.append(article.text)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # For larger numbers, use a balanced approach with just 2-3 threads\n",
    "    # Create 2-3 batches\n",
    "    num_batches = min(3, len(files_to_process))\n",
    "    batch_size = len(files_to_process) // num_batches\n",
    "    batches = [files_to_process[i:i + batch_size] for i in range(0, len(files_to_process), batch_size)]\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=len(batches)) as executor:\n",
    "        batch_results = executor.map(process_file_batch, batches)\n",
    "        \n",
    "        for batch_result in batch_results:\n",
    "            results.extend(batch_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f6282b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs3 = get_sentiment3(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
